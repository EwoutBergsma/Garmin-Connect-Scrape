{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2da19b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44562d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_parse_html_files(directory):\n",
    "    \"\"\"\n",
    "    Load and parse all HTML files in the specified directory.\n",
    "    Args:\n",
    "        directory (str): The path to the directory containing HTML files.\n",
    "    Returns:\n",
    "        list: A list of BeautifulSoup objects representing the parsed HTML files.\n",
    "    \"\"\"\n",
    "    html_files = glob.glob(os.path.join(directory, '*.html'))\n",
    "    if not html_files:\n",
    "        raise FileNotFoundError(f\"No HTML files found in directory: {directory}\")\n",
    "    else:\n",
    "        print(f\"Found {len(html_files)} HTML files in directory: {directory}\")\n",
    "\n",
    "    soups = []\n",
    "    # Open each HTML file\n",
    "    for file_path in html_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                soups.append(BeautifulSoup(content, 'html.parser'))\n",
    "                # You can process the soup object as needed\n",
    "                print(f\"Processed file: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    return soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c092b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 HTML files in directory: html\n",
      "Processed file: html\\09_04_2025.html\n",
      "Processed file: html\\10_04_2025.html\n",
      "Processed file: html\\13_04_2025.html\n",
      "Start time: 23:51:00, End time: 08:24:00\n",
      "Start time: 00:33:00, End time: 08:24:00\n",
      "Start time: 22:29:00, End time: 07:42:00\n"
     ]
    }
   ],
   "source": [
    "def find_begin_and_end_timestamps(soup):\n",
    "    \"\"\"\n",
    "    Find the first and last timestamps of the graph (see README.md) in the HTML soup object.\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object containing the HTML content.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the first and last timestamps as numpy datetime64 objects.\n",
    "    \"\"\"\n",
    "\n",
    "    # Example of div with (start) time: <div class=\"    flexboxgrid_colXs__2BUM1      flexboxgrid_collapse__v6Hdm  \"><span>12:33 AM</span><span class=\"SleepTimeEditors_pencilIcon__ejpUi\"><i class=\"icon-pencil undefined\" style=\"font-size: 14px;\"></i></span></div>\n",
    "    time_elements = soup.find_all(\"div\", class_=\"flexboxgrid_colXs__2BUM1\")\n",
    "    time_elements = [time_element.text.strip() for time_element in time_elements]\n",
    "    #   # Convert list of text strings (e.g. \"12:33 AM\") to list of datetime.time objects\n",
    "    time_elements = [datetime.strptime(time_element, \"%I:%M %p\").time() for time_element in time_elements]  # Convert text string (e.g. \"12:33 AM\")\n",
    "\n",
    "    # Check if we have exactly two time elements (start and stop time)\n",
    "    if len(time_elements) != 2:\n",
    "        raise ValueError(f\"Found {len(time_elements)} time elements, expected exactly two (i.e. start and stop time).\")\n",
    "    \n",
    "    return time_elements\n",
    "\n",
    "# Test the functions\n",
    "soups = load_and_parse_html_files('html')\n",
    "for soup in soups:\n",
    "    start_time, end_time = find_begin_and_end_timestamps(soup)\n",
    "    print(f\"Start time: {start_time}, End time: {end_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f13dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 graphs found in the HTML soup.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found 2 graphs HTML soup, expected exactly one (i.e. the sleep graph).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rectangles), \u001b[33m\"\u001b[39m\u001b[33mrectangles found in the graph.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# Placeholder for the actual implementation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mget_timestamps_for_each_sleepcycle\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoups\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mget_timestamps_for_each_sleepcycle\u001b[39m\u001b[34m(soup, start_time, end_time)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(graph), \u001b[33m\"\u001b[39m\u001b[33mgraphs found in the HTML soup.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(graph) != \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(graph)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m graphs HTML soup, expected exactly one (i.e. the sleep graph).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m graph = graph[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# Get the first (and only) graph element\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(graph)\n",
      "\u001b[31mValueError\u001b[39m: Found 2 graphs HTML soup, expected exactly one (i.e. the sleep graph)."
     ]
    }
   ],
   "source": [
    "def get_timestamps_for_each_sleepcycle(soup, start_time, end_time):\n",
    "    \"\"\"\n",
    "    Get the timestamps for each sleep cycle from the parsed HTML soups. This is done by assessing the relative locations of the coloured rectangles in the graph. Returns start and end times of each sleep cycle, including which sleep cycle label.\n",
    "    Args:\n",
    "        soup: BeautifulSoup: The BeautifulSoup object containing the HTML content.\n",
    "        start_time: datetime.time: The start time of the sleep.\n",
    "        end_time: datetime.time: The end time of the sleep.\n",
    "    Returns:\n",
    "        list: A list of tuples, each containing the start and end timestamps and label of a sleep cycle.\n",
    "    \"\"\"\n",
    "    # Each rectangle is a <g></g> with multiple <g></g> elements, each with multiple <path></path> elements inside it. First find that upper <g></g>, for example with class=\"highcharts-series-group\"\n",
    "    graph = soup.find_all(\"g\", class_=\"highcharts-series-group\")\n",
    "\n",
    "    if len(graph) != 2:\n",
    "        raise ValueError(f\"Found {len(graph)} graphs HTML soup, expected exactly two (i.e. the pie chart at the top of the page, and the sleep graph of interest).\")\n",
    "\n",
    "    graph = graph[1]  # Get the second graph, which is the sleep graph (of interest)\n",
    "\n",
    "    # Then in that graph find all <g></g> that construct the rectangles of the sleep cycles, for with class such as: [\"\"highcharts-series highcharts-series-0 highcharts-area-series\", \"highcharts-series highcharts-series-1 highcharts-area-series\"]\n",
    "    rectangles = []\n",
    "    i = 0\n",
    "    while True:\n",
    "        rectangle = graph.find_all(\"g\", class_=f\"highcharts-series highcharts-series-{i} highcharts-area-series\")\n",
    "        if not rectangle:\n",
    "            break\n",
    "        rectangles.append(rectangle)\n",
    "        i += 1\n",
    "    \n",
    "    print(len(rectangles), \"rectangles found in the graph.\")\n",
    "\n",
    "    return None # Placeholder for the actual implementation\n",
    "\n",
    "get_timestamps_for_each_sleepcycle(soups[0], start_time, end_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
